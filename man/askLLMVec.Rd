% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm.R
\name{askLLMVec}
\alias{askLLMVec}
\title{Ask an LLM a question about an article summary}
\usage{
askLLMVec(model, prompt, article, endpoint = "localhost", verbose = TRUE)
}
\arguments{
\item{model}{Model name(s) to ask (must be available on all endpoints)}

\item{prompt}{Prompt name defined in the settings}

\item{article}{A data frame of one or more articles to ask about}

\item{endpoint}{Endpoint to send the query to}

\item{verbose}{Print HTTP error codes if the query fails

The endpoint_name is from the settings.xml file.
If you haven't changed it, the default "localhost"
points to http://localhost:11434, the default for ollama.

The valid model names are fetched when initially reading the settings.
If you have installed a new model later, re-run publlamaInit().}
}
\description{
Ask an LLM a question about an article summary
}
