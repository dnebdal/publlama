% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm.R
\name{askLLMVec}
\alias{askLLMVec}
\title{Ask an LLM a question about an article summary}
\usage{
askLLMVec(
  model,
  prompt,
  article,
  endpoint = "localhost",
  qlen = 1,
  verbose = FALSE,
  retries = 10,
  force.new = FALSE
)
}
\arguments{
\item{model}{Model name(s) to ask (must be available on all endpoints)}

\item{prompt}{Prompt name defined in the settings}

\item{article}{A data frame of one or more articles to ask about}

\item{endpoint}{Endpoint to send the query to}

\item{qlen}{Number of queries to queue up per runner}

\item{verbose}{Print HTTP error codes if the query fails}

\item{retries}{Retry this many times if a query fails (sleeping randomly 1-5 sec between each)}

\item{force.new}{Send question to LLM even if this question+model+pmid combo has already been done
#'
The endpoint_name is from the settings.xml file.
If you haven't changed it, the default "localhost"
points to http://localhost:11434, the default for ollama.

The valid model names are fetched when initially reading the settings.
If you have installed a new model later, re-run publlamaInit().

By default, questions that have already been asked (that is, there is an answer
in the DB for the same model, prompt and pubmed id) use that existing answer.
If there are multiple, the newest answer is returned.
To override this and ask again, set force.new=TRUE.}
}
\description{
Ask an LLM a question about an article summary
}
