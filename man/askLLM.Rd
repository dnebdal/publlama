% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm.R
\name{askLLM}
\alias{askLLM}
\title{Ask an LLM a question about an article summary}
\usage{
askLLM(model, prompt, summary, endpoint_name = "localhost", verbose = TRUE)
}
\arguments{
\item{model}{A model name (e.g. "phi4:latest") that's available on the endpoint}

\item{prompt}{The question to ask}

\item{summary}{The article summary to ask about}

\item{endpoint_name}{Endpoint to send the query to}

\item{verbose}{Print HTTP error codes if the query fails

The endpoint_name is from the settings.xml file.
If you haven't changed it, the default "localhost"
points to http://localhost:11434, the default for ollama.

The valid model names are fetched when initially reading the settings.
If you have installed a new model later, re-run publlamaInit().}
}
\description{
Ask an LLM a question about an article summary
}
